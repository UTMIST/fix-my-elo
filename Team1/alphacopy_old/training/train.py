import numpy as np
import chess
from model import build_model
from mcts import MCTS
import config
from src.export_board import encode_board

def self_play(model):
    """
    AI plays against itself to generate training data.
    Returns: A list of (Input, Policy_Target, Value_Target)
    """
    game_data = []
    board = chess.Board()
    mcts = MCTS(model)

    while not board.is_game_over():
        # run MCTS to get the best move probabilities
        visit_counts = mcts.search(board)
        
        # convert visit counts to a probability vector (Policy Target)
        policy_target = np.zeros(config.ACTION_SPACE_SIZE)
        total_visits = sum(visit_counts.values())
        
        for move, visits in visit_counts.items():
            idx = mcts._map_move_to_int(move)
            policy_target[idx] = visits / total_visits

        # save state data
        state_input = encode_board(board)
        game_data.append([state_input, policy_target, None]) # Value is unknown yet

        # pick a move (Weighted random based on visits for exploration)
        move = max(visit_counts, key=visit_counts.get) 
        board.push(move)

    # game over at this point
  
    result = 0 
    if board.result() == "1-0": result = 1  # if White won, White moves get +1, Black moves get -1
    elif board.result() == "0-1": result = -1
    
    #fill in the 'Value' for each step of the game
    final_dataset = []
    for i, (state, policy, _) in enumerate(game_data):
        # alternate 1 and -1 depending on whose turn it was
        player_turn = (i % 2 == 0) # assuming White starts
        current_value = result if player_turn else -result
        final_dataset.append((state, policy, current_value))

    return final_dataset

def train_pipeline():
    model = build_model()
    
    for epoch in range(config.EPOCHS):
        print(f"Starting Epoch {epoch+1}...")
        dataset_inputs = []
        dataset_policies = []
        dataset_values = []

        # SELF-PLAY
        for _ in range(config.SELF_PLAY_GAMES):
            game_data = self_play(model)
            for s, p, v in game_data:
                dataset_inputs.append(s)
                dataset_policies.append(p)
                dataset_values.append(v)

        # training phase
        inputs = np.array(dataset_inputs)
        policies = np.array(dataset_policies)
        values = np.array(dataset_values)

        # fit to the data generated by the MCTS
        model.fit(
            x=inputs, 
            y={'policy': policies, 'value': values}, 
            batch_size=config.BATCH_SIZE, 
            epochs=1
        )
        
        # save progress
        model.save(f"weights/checkpoint_{epoch}.h5")

if __name__ == "__main__":
    train_pipeline()